\chapter{Cluster labeling}
A major problem in text analysis is to determine the topics in a text collection and identify the most important or significant relationships between them. Clustering and visualizations (tag clouds) are key analysis methods in order to address this problem.  In this chapter, we discuss three novel algorithms for cluster labeling, and in Chapter~\ref{sec:tagclouds} we shall introduce tag clouds as a visualization mean in \gls{IR} systems. \\

\section{Clustering}
Clustering is an \gls{IR} method that groups objects (documents) together based on their similarities in so called clusters. Such methods are often applied in the post processing phase of \gls{IR}~(fig.~\ref{fig:text_analysis}) for visualization of retrieved results. For example, similar search results can be grouped together during presentation of results in search engines. This should help users gain a quick overview of the retrieved results. It is important to choose suitable labels of the categories defined in this way, so that they are usable to human users. The process of cluster labeling creates labels for these clustered groups of objects.  \\

\subsection{Clustering classification}
A large variety of clustering algorithms exists, and they are usually classified according to the characteristic of their output structure. Jain et al.~\cite{clusteringJain99} make a classification based on the following properties: \\

\textbf{Flat vs. hierarchical clustering} \\
Flat clustering creates a flat set of clusters without relations between clusters due to some structure. Hierarchical clustering creates a hierarchy of clusters, with parents and children, in a tree-like manner. \\

\textbf{Hard vs. soft clustering}\\
Another important distinction can be made between hard and soft clustering algorithms. If each document is assigned to a single cluster only, we have hard clustering. Otherwise, clustering is soft, and it assigns a document as a distribution over all clusters. This means that each document has a fractional membership in several clusters. \gls{LSA}~(Chapter~\ref{sec:lsa}) is a good example for a soft clustering algorithm. K-means is a popular example for hard clustering. \\


\textbf{Monothetic vs. polythetic clustering} \\
Based on how documents are assigned to different clusters, clustering can be divided into \textit{monothetic} and \textit{polythetic}~\cite{hierarchMonotheticClustering2004}. Monothetic algorithms assign a document to a cluster based on a single feature, while polythetic algorithms classify documents by overall degree of similarity/difference calculated over many properties. This makes monothetic clustering well suited for generating hierarchies and browsing search results, since a single feature, common to all documents in the cluster, describe each cluster. Users understand easily clusters generated in this way. K-means is an example of polythetic document clustering algorithm, where each cluster is described by several words of phrases. \\


\textbf{Partial vs. complete clustering} \\
Complete clustering assigns each document to a cluster. Partial clustering, on the other hand, doesn't assign all document to clusters. \\


Carpineto et al.~\cite{surveyWebClusteringEngines2009} claim that there is a difference between clustering of data sets, and clustering of search results, returned from search engines: "in search results clustering description comes first". Thus, in contrast to the classical categorization of clustering algorithms,  they propose a classification based on how well the clustering algorithms can generate sensible, comprehensive and compact cluster labels, and divide algorithms in the following three categories:  \\

\textbf{Data-centric algorithms} \\
Data-centric algorithms were the first ones to be implemented for cluster labeling. Here, clustering is done with some general clustering algorithm, and terms which are close to the cluster centroids are nominated as cluster labels. Cluster centroids are a collection of independent terms from all documents not related to each other. In order to define the terms from cluster centroid, one uses a frequency measure, such as $tf-idf$~(fig.~\ref{lsa:tf_idf}). We present \gls{WCC}~(see~\ref{clustering:WCC}) as an example for a data-centric clustering algorithm.  \\

\textbf{Description-aware algorithms} \\
Description-aware algorithms assign labels during clustering process. Using monothetic term selection, one nominates the labels which are interpretable to human users. Suffix Tree Clustering is an example of such an algorithm, introduced by Zamir and Etzioni~\cite{suffixTreeClustering99}. Clustering and labeling are closely related, and labeling influences the clustering process. Therefore, it is not possible to combine the labeling with any clustering algorithm. \\

\textbf{Description-centric algorithms} \\
These algorithms operate on the principle \textit{"description comes first"}, and in this sense are the opposite of the data-centric algorithms. The goal here is to find meaningful cluster labels. If there is not suitable label found, the cluster is useless for the users, and therefore is discarded. Description-centric algorithms are mainly applied for clustering of search results~(see~\cite{surveyWebClusteringEngines2009}). We will introduce next two examples of such algorithms - Descriptive k-means~(see Section~\ref{clustering:descriptive_k-means}) and LINGO~(see Section~\ref{clustering:lingo}). \\ 

\subsection{Clustering algorithm??}
\label{clustering:algos}
Which method to present? K-means or hierarchical agglomerative clustering? \\
We implement in this work ? clustering algorithm, therefore what follows is an overview of this \gls{IR} method. \\


\section{Cluster labeling}
There is not a commonly accepted definition of cluster labeling in literature, as it is a relatively young field of research, as compared to clustering. \\

Assume that a categorization of a document set is determined using an unsupervised approach. To present this categorization to a user, it is convenient to label the individual categories with characteristic terms. These terms, called \textit{category labels}, should characterize the content of the associated category with respect to the remaining categories. This property implies that cluster labeling should summarize a category's content and that it should discriminate a category from the other categories. This section states desired properties of category labels and reviews three algorithms which generate such labels. It further makes a  proposition for improvement of \gls{WCC}~(section~\ref{clustering:WCC}) topic identification method, and evaluates it using hierarchical clustering. \\


When performing classification by clustering, the cluster labels are usually manually created by humans. However, this is a  very expensive approach. It is sensible to find and algorithm for automatically identifying topic labels or cluster labels. Therefore, we have chosen to present three new algorithms for cluster labeling, which offer improvements in different aspects of the classical methods used until now. We further implement the Weighted Centroid Covering algorithm  by Stein and zu Eissen\cite{Stein04topicidentification}, and make an evaluation of its performance. We also propose to improve \gls{WCC} using external semantic knowledge. Therefore, we have developed a domain ontology for CoreMedia \gls{CMS} domain, which is the use case of this work. \\

Some clustering algorithms attempt to find a set of labels first and then build
(often overlapping) clusters around the labels, thereby avoiding the problem
of labeling altogether (Zamir and Etzioni 1999, Käki 2005, Osi ´nski andWeiss
2005). \\

\subsection{Formal framework for cluster labeling algorithms}
No uniformly agreed upon formal framework exists still for cluster labeling algorithms.  Stein and Meyer Zu Eissen~\cite{Stein04topicidentification} have given their formal definitions for cluster labeling algorithms. We base our definition on this source. \\ 


\subsection{Algorithms for cluster labeling}
We shall present three novel algorithms for cluster labeling. 

\subsubsection{Weighted Centroid Covering}
\label{clustering:WCC}
Weighted Centroid Covering - 2004

\subsubsection{Descriptive k-means}
\label{clustering:descriptive_k-means}
Descriptive k-means - 2007 \\

\subsubsection{Lingo}
\label{clustering:lingo}
Lingo - 2004 \\

\section{Cluster labeling using external knowledge}
The best scenario is that cluster labels should present a conceptualization of the documents in text corpus. This is not achieved by the algorithm presented. Technically, a hierarchical clustering algorithm can construct from each Document set $D$ a category tree. However, the labeling based on this hierarchical clustering will be far from a semantical taxonomy. This weakness of the algorithm presented can be corrected by using an external classification knowledge, e.g. an upper-level ontology. \\

Unfortunately, domain ontologies usually have coverage limitations because not all the terms of the domain are included in the ontology.\\

Classical clustering methods are not able to deal with the semantics of the linguistic values of the objects (notions, texts). In this paper, a general methodology to incorporate this knowledge into the cluster labeling process has been presented. \\

We believe that the weaknesses of topic identification algorithms in categorizing search engines could be overcome if external classification knowledge were brought in. We now outline the ideas of such an approach where both topic descriptors and hierarchy information from an upper ontology are utilized. \\

 Then, topic identification is based on the following paradigms:\\
1. Initially, no hierarchy (refines-relation) is presumed among the C 2 C. This is in accordance with the observations made in [Ertöz et al. 2001].\\
2. Each category C 2 C is associated to its most similar set O 2 O. If the association is unique, $ T o (O)$ is selected as category label for C.\\
3. Categories which cannot be associated uniquely within O are treated by a polythetic, equivalence-presuming labeling strategy in a standard way.
In essence, finding a labeling for a categorization C using an ontology O means
to construct a hierarchical classifier, since one has to map the centroid vectors of the clusters C 2 C onto the best-matching O 2 O. Note that a variety of machine learning techniques has successfully been applied to this problem; they include Bayesian classifiers, SVMs, decision trees, neural networks, regression techniques, and nearest neighbor classifiers.	\\
