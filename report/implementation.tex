\chapter{Implementation and evaluation}
\label{sec:implementation}

\begin{summary}
This chapter reports the implemented solution for the given thesis problem, presents experimental results, and discusses its advantages and disadvantages.
\end{summary}


It is a challenge by itself to come up with a sensible evaluation set for an IR implementation. ... Define here precision, recall, measures , how to measure LSA performance with  diff. k, clusters, cluster labelling. \\


The document collection consists of guides and manuals about CoreMedia \gls{CMS} 5.2.\\

\section{Measures for evaluation}
Evaluation in \gls{IR} is based on measures. We outline several measures used with commonly used in \gls{IR} systems to measure performance. \\
\subsection{Evaluation of LSA}
Presision: \\
Recall: \\
\subsection{Evaluation of algorithms for cluster labeling}
F-measure:\\

\section{LSA implementation}
\label{sec:implementation:lsa_impl}
\gls{LSA} was applied to a collection of 11818 words in 4000 documents, all of which describe CoreMedia \gls{CMS} 5.2. The algorithm took  63552 ms for preprocessing and indexing of the whole document collection. \\

For the implementation of LSA this work uses the open LSA library which is part of Semantic Spaces Project\cite{S-Space}. It is developed at the Natural Language Processing Group at the University of California at Berkley (UCLA)\footnote{\url{http://code.google.com/p/airhead-research/}}.\\

The real difficulty of LSA is to find out how many dimensions to remove - the problem of dimensionality.\\

TODO:test if inluding only terms that occur in more than one document improved \gls{LSA} performance with respect to generating precise tag clouds.\\

\subsection{Latest development in the field of LSA}

LSAView is a tool for visual exploration of latent semantic modelling, developed at Sandia National Laboratories \cite{CrDuSh09}.\\

at the end- improvements of lsa with the basics explained.\\

\section{Tag Cloud implementation}
\label{sec:implementation:tag_cloud}
The implemented open source library used for tag cloud generation is called Opencloud\footnote{\url{http://opencloud.mcavallo.org/}}, and is provided by Marco Cavallo.\\

\section{Tools used}
\label{sec:implementation:tools_used}
Airhead Research\footnote{\url{http://code.google.com/p/airhead-research/}} project was used as a semantic spaces Package which provides a java-based implementation of \gls{LSA}.\\

Apache Lucene\footnote{\url{http://lucene.apache.org/java/3_0_2/}} was used as an indexing and search library.

\section{Advantages and drawbacks}
\label{sec:lsa:adv_disadv}

why am i using lsa instead of lda for example?\\

\begin{enumerate}
\item PLSA - characteristics, advantages, disadvantages
\item LDA - characteristics, advantages, disadvantages
\end{enumerate}



\section{Experimental evaluation}
Example for presentation and evaluation of results, IR system:~\cite{SpamFilteringLSI2008} \\
\begin{itemize}
\item Data set
\item Experimental setup
\item Analysis of data matrices
\item True/false positive rates
\item Feature reduction
\end{itemize}

\subsection{Document set}

\begin{center}
\begin{tabular}{l l  c}
\hline
\multicolumn{3}{c}{Document set}\\
\hline
Category Id & Category & Document \# \\
\hline
1 & session, connection & 5 \\
2 & server & 5 \\
3 & publication, workflow & 5 \\
\hline
%\caption[Document set]{Overview of the constructed document set}
\end{tabular}
\end{center}

The document set used for evaluation consists of 3 categories, and 15 documents - each category has 5 documents, and the documents have different length. Table~\ref{eval:data_set} gives an overview of the constructed data sets. The document preprocessing involves parsing and stop word removal according to standard stop word lists. \\

% TODO: correct text wrapping - fix column width for document text
\begin{landscape}
\begin{table} [!ht]
\begin{center}
\begin{tabular} { c c lp{10cm} }
\hline
Doc. Id & Cat. Id & Document \\
\hline
1 & 1 & The session that is created while the connection is opened is also known as the connection session. \\
2 & 1 & The sessions of the connected clients will be closed and no more content changes are possible. \\
3 & 1 & The previous code fragment shows how a second session is created from an existing connection. \\
4 & 1 & Multiple sessions show their greatest potential in trusted applications which receive help in restricting user views while maintaining a shared cache.  \\
5 & 1 & Having opened connection, all actions are executed on behalf of the single user whose credentials where provided when logging in. \\
\hline
6 & 2 & The state of the Master Live Server must be younger than the state of the Slave Live Server.  \\
7 & 2 & The rewrite module checks the availability of the requested file and, in the negative case, passes the request on to the Active Delivery Server. \\
8 & 2 & The directory layout of the Active Delivery Server has changed as well as the format of the configuration file. \\
9 & 2 & The CoreMedia Content Server is a central component that manages the content repository and the user authorization. \\
10 & 2 & The CoreMedia Content Management Server is the production system used to create and administrate content. \\
\hline
11 & 3 & If the database does not allow to take online-backups ensure that all publications are finished and that the last publication was successful. \\
12 & 3 & The third task in the workflow aims to check if the change set is empty. Then, no publication is necessary and the workflow can be finished.  \\
13 & 3 & This element is used to define which information should be shown in the columns of the workflow list at the left side of the workflow window. \\
14 & 3 & Publication will be executed when finishing the task after all resources in the change set have been approved. \\
15 & 3 & The CoreMedia Workflow installation comes with four predefined workflows which cover the publication of resources. \\
\hline
\end{tabular}
\caption[Document set]{Document set used for evaluation}
\label{eval:data_set}
\end{center}
\end{table}
\end{landscape}


\subsection{Cluster labeling evaluation}

To evaluate the quality of WCC, Popescul and Ungar’s method, and the standard
keyword extraction algorithm RSP, we downloaded 150 documents from the digital library Citeseer, each of which containing author-defined keywords.

To evaluate the WCC algorithm, we prepared a document set with author-defined keywords. The evaluation idea is to cluster the documents, label the resulting clustering, compute f1-f4, and measure to which extent the identified topic labels appear in the keyword list of at least one document of the associated cluster. To measure the extent, standard precision and recall values can be chosen. Note, however, that the precision value is more important in our scenario since usually only a few words (about one to five) are presented to a user. \\

The clustering was done on the results from queries used for evaluation of \gls{LSA} (??).  \\
The clustering was done on the prepared data set~\ref{eval:data_set}, used for evaluation of the \gls{IR} methods implemented in this work. \\

Figure 3.15 shows precision and recall curves for \gls{WCC} depending on the number of extracted words per label. The values are averaged over 10 retries of the experiment, each time redrawing a new set of documents. The precision curve shows that one to five cluster labels can be identified at very high precision rates. (??check with my results?) \\

\subsection{Results}


\section{LSA evaluation from IR book}
Dumais (1993) and Dumais (1995) conducted experiments with LSI on
TREC documents and tasks, using the commonly-used Lanczos algorithm
to compute the SVD. At the time of their work in the early 1990’s, the LSI
computation on tens of thousands of documents took approximately a day
on one machine. On these experiments, they achieved precision at or above
that of the median TREC participant. On about 20% of TREC topics their
system was the top scorer, and reportedly slightly better on average than
standard vector spaces for LSI at about 350 dimensions. Here are some conclusions
on LSI first suggested by their work, and subsequently verified by
many other experiments. \\
• The computational cost of the SVD is significant; at the time of this writing,
we know of no successful experiment with over one million documents.
This has been the biggest obstacle to the widespread adoption to
LSI. One approach to this obstacle is to build the LSI representation on a
randomly sampled subset of the documents in the collection, following
which the remaining documents are “folded in” as detailed with Equation
(18.21). \\
• As we reduce k, recall tends to increase, as expected. \\
• Most surprisingly, a value of k in the low hundreds can actually increase
precision on some query benchmarks. This appears to suggest that for a
suitable value of k, LSI addresses some of the challenges of synonymy. \\
• LSIworks best in applicationswhere there is little overlap between queries
and documents. \\
The experiments also documented some modes where LSI failed to match
the effectiveness of more traditional indexes and score computations. Most
notably (and perhaps obviously), LSI shares two basic drawbacks of vector
space retrieval: there is no good way of expressing negations (find documents
that contain german but not shepherd), and noway of enforcing Boolean
conditions. \\
LSI can be viewed as soft clustering by interpreting SOFT CLUSTERING each dimension of the
reduced space as a cluster and the value that a document has on that dimension
as its fractional membership in that cluster. \\
