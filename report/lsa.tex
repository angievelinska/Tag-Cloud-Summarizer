\chapter{Latent Semantic Analysis}
\label{sec:lsa}

\begin{summary}
This chapter gives the theoretical foundations of \gls{LSA} since it is used in this project as a method for defining the main concepts in text documents.
\end{summary}
 

\section{Overview}


\gls{LSA} was developed at the end of 1980s to address certain deficiencies in Information Retrieval, caused by synonymy and polysemy. Synonymy is the case when several words describe the same concept. Polysemy is when words have more than one distinct meaning. \\

\gls{LSA} method is applied in four main steps. First, a words-by-documents matrix, constructed using the documents in the text collection. This matrix is sparse, as not all words occur in all documents.\\

%
% initial sparse matrix A
%
\begin{equation}
A=
\begin{bmatrix}\label{lsa:sparse_matrix_A}
 a_{11}& a_{12}& \cdots& a_{1n} \\
 \vdots& \vdots& \ddots& \vdots \\ 
 a_{m1}& a_{m2}& \cdots& a_{mn}
\end{bmatrix}\\
\end{equation}\\

Each word (or term) is a row in the matrix, and each document - a column. The size of the matrix is \text{\bf{m x n}}, where {\bf m} is the number of documents in the text collection, and {\bf n} is the number of terms. As a second step, the number of term occurences in each document is transformed into a weight using a weight function, such as entropy, term frequency, inverse document frequency. These weights are represented by the $ a_{ij} $ entries in the matrix in~\ref{lsa:sparse_matrix_A}. After the initial pre-processing, and as a third step of \gls{LSA}, the rank of the matrix is reduced by applying a method of matrix decomposition, called \gls{SVD}. The initial matrix $A$ is large and sparse, such that decomposition reduces the number of rows and columns to a certain parameter {\bf k}, defined empirically. The result from applying \gls{SVD} to the terms-by-documents matrix $A$ are tree matrices $U$, $S$ and  $V$, as in~\ref{lsa:svd}. \\

%
% SVD decomposition in three matrices
%
\begin{equation}\label{lsa:svd}
A=USV^{T}\\
\end{equation}\\

$U$ is a ...\\
 $V$ is ...\\
and $S$ is ...\\
After the initial matrix $A$ is decomposed by \gls{SVD}, all but the highest $k$ valued of $S$ are set to $0$. The resulting reduced matrix is the semantic space of the text collection.\\

And the final step of applying \gls{LSA} is to the compute similarities between entities in the semantic space. This includes computing similarities between queries posted on the document collection, and the documents in the semantic space. In this case the query $q$ has to be ''translated'' as a document from the semantic space, by using~\ref{lsa:query}.\\

%
% Query translation for LSA
%
\begin{equation}\label{lsa:query}
q = q^{T}U_{k}S_{k}^{-1}\\
\end{equation}\\


\section{Singular Value Decomposition}

\gls{SVD} is an unique decomposition of \\


\gls{LSA} constructs a term-by-document matrix based on term occurrence, and uses a given similarity measure to find out the distance between vectors (documents) in the semantic space it generates.\\

There are three main factors that can influence the performance of LSA\cite{Nakov_weightfunctions}\cite{NakovBetterResultsLSI}:\\
\begin{itemize}
\item Frequency matrix transoformations (choice of weighting function)
\item Choice of dimensionality
\item Text preprocessing prior to SVD, choice of similarity measure (???)
\end{itemize}

Further, the choice of dimensionality is dependent upon the matrix transformations performed, as pointed out by Nakov\cite{NakovBetterResultsLSI}.\\


\section{Latest development in the field of LSA}
LSAView is a tool for visual exploration of latent semantic modelling, developed at Sandia National Laboratories \cite{CrDuSh09}.\\

at the end- improvements of lsa with the basics explained.\\
why am i using lsa instead of lda for example?\\
\section{plan}
\label{sec:lsa:plan}
1. text processing and peculiarities; stemming, lemmatization, stop-wording\\
2. lsa and basics\\
3. weighting functions and their effect ot LSA results \cite{Nakov_weightfunctions}. \\
4. lsa used for information retrieval; lsa used for defining the main concepts in texts. precision vs. recall. \\
(first explain the basics of LSA, then explain how factors can influence lsa)\\
Several factors influence the quality of results which LSA delivers. These factors are pre-processing (removal of stop-words, stemming, lemmatization), frequency matrix transformations, choice of dimensionality, choice of similarity measure.\\
A study by Nakov, Popova, Mateev\cite{Nakov_weightfunctions} has summarized the influence of those factors on LSA, and has concluded that...\\

\section{storage}
Text Processing and LSA\\

Text processing:\\
-	retrieve documents from DB\\
-	tokenize texts\\
-	stem/lemmatize texts - this drops off as we will use the terms as a part of a tag cloud\\
-	stop wording\\
-	build SVD\\
-	post queries on the matrix\\
\\
The document collection consists of guides and manuals about CoreMedia Content Mangement System 5.2.\\
\\
Improving performance of LSA information retrieval method includes tf*idf weighting scheme, relevance feedback by implementing Tag Cloud, and choosing the number of dimensions for the reduced spacing. Stemming as a method for LSA improvement is not applied, as investigations showed at most modest improvements with this method.\\
\\
Library/implementation used for LSA is S-Spaces from Airhead Research project of UCLA (University of California at Los Angeles). The implemented algorithm for SVD is Lanczos, ported from SVDLIBC implementation by Doug Rohde from Tennessee University.\\
\\
Use the paper "Weight functions impact on LSA performance" by Preslav Nakov, Antonia Popova, Plamen Mateev - very nice concise description of LSA + analysis.\\
\\
IMPORTANT\\
I should test entropy and idf , as sometimes entropy global weighting function has a better performance.\\ 
\\
For text processing, Snowball project is used, from the laboratory of Martin Potter, the author of the infamous Porter Stemming algorithm.
!!! No stemming or lemmatization should be done on the input document collection, as the resulting terms/tags from LSA will be used in a TagCLoud!\\
\\

11818 words in word space\\
 63552ms to run LSA on 4000 documents\\
and IDEA blocks \\
\\
Due to the problem above, the process of SVD calculation has to be performed in a multi-threaded way, and the project has to be optimized with respect to performance, in order to be able to successfully run.\\
Keep only wht words common to at least 2 documents???\\

\section{Alternative approaches for LSA}
\begin{enumerate}
\item PLSA - characteristics, advantages, disadvantages
\item LDA - characteristics, advantages, disadvantages

