\chapter{Evaluation}
\label{chapter:evaluation}

Evaluation in \gls{IR} is based on measures. As this work implements techniques for information retrieval, several wide-spread measures are described next, used commonly in \gls{IR} systems to evaluate performance. Then, the implemented tests are presented, and finally the obtained test results are discussed. \\


\section{Measures for evaluation}
In order to perform an evaluation of \gls{IR} systems, one needs in the simplest case:
\begin{itemize}
\item a document collection, 
\item a test suite of information needs, or a set of queries,
\item a set of relevance judgments to define for each query-document pair weather the document is relevant or not relevant for the query posed.
\end{itemize} 

Relevance is assessed relative to an information need, not a query. A document is considered relevant if it addresses the users' information need, not only if it contains all the words given in the query~(Manning et al.~\cite{Mann08}). When short queries (composed of one word for example) are posted, it is difficult to define the information need of users. But nevertheless, the user has one, and the retrieved search results are evaluated based on his or her information need. \\

The two most frequent and basic measures for evaluation of \gls{IR} systems are \textit{precision} and \textit{recall}. For the simple case of a system which returns a result set of document to a query, they are defined:  \\

\subsubsection{Precision}
\begin{equation}
\mbox{\textbf{precision}} = \frac{\mbox{number of relevant documents retrieved}} {\mbox{total number of relevant documents}}
\end{equation}


\subsubsection{Recall}
\begin{equation}
\mbox{\textbf{recall}} = \frac{\mbox{number of relevant documents retrieved}} {\mbox{total number of retrieved documents}}
\end{equation}


\section{Evaluation of LSA implementation}

\subsubsection{Document set for evaluation}
\label{sec:eval:doc_set_lsa}
\gls{LSA} was applied to a collection of 11818 words in 4000 documents, all of which describe CoreMedia \gls{CMS} 5.2. The algorithm took 63552 ms for preprocessing and indexing of the whole document collection. \\

For the implementation of LSA this work uses the open LSA library which is part of Semantic Spaces Project\cite{S-Space}. It is developed at the Natural Language Processing Group at the University of California at Berkley (UCLA)\footnote{\url{http://code.google.com/p/airhead-research/}, accessed December, 2010}.\\

The real difficulty of LSA is to find out how many dimensions to remove - the problem of dimensionality.\\

TODO:test if inluding only terms that occur in more than one document improved \gls{LSA} performance with respect to generating precise tag clouds.\\

\subsubsection{Data set}
It is a challenge by itself to come up with a sensible evaluation set for an IR implementation. ... Define here precision, recall, measures , how to measure LSA performance with  diff. k, clusters, cluster labelling. \\

The document collection consists of guides and manuals about CoreMedia \gls{CMS} 5.2.\\

This work has been carried out using documents from the domain of CoreMedia \gls{CMS} 5.2 as an evaluation set, developed at CoreMedia AG, Hamburg. \\


\subsubsection{Dimensionality reduction parameter in LSA}
Certain factors influence the performance of LSA, as already discussed in section~\ref{sec:lsa:factors_infl_lsa}. 
\begin{itemize}
\item \textit{Preprocessing} - before constructing a semantic space using \gls{LSA}, stop words were removed from the evaluation document set. Stemming is not done, as terms from the semantic space are later used for the tag cloud generation.
Stop words and punctuation are removed. The data model used in DocMachine stores the documents as XML files, and CoreMedia \gls{CMS} Unified API\footnote{\url{https://documentation.coremedia.com/servlet/content/241548?language=en&version=5.2&book=coremedia:///cap/content/241548}} is used to strip the markup, in order to access the plain text~(listing~\ref{strip_markup}). \\

\item \textit{Frequency matrix transformation} - log-entropy weight functions~(section~\ref{lsa:log-entropy-section}) were chosen, due to their better performance as compared to other measures (reported by Nakov et al.~\cite{Nakov_weightfunctions}).
\item \textit{Dimensionality reduction parameter \textbf{k}} - it is defined based on the experiments carried out using the document set for evaluation from CoreMedia \gls{CMS} domain, described in section~\ref{sec:eval:doc_set_lsa}. 

\end{itemize}


\subsubsection{LSA evaluation results}
• As we reduce k, recall tends to increase, as expected. \\
• Most surprisingly, a value of k in the low hundreds can actually increase
precision on some query benchmarks. This appears to suggest that for a
suitable value of k, LSI addresses some of the challenges of synonymy. \\
* The experiments also documented some modes where LSI failed to match
the effectiveness of more traditional indexes and score computations. Most
notably (and perhaps obviously), LSI shares two basic drawbacks of vector
space retrieval: there is no good way of expressing negations (find documents
that contain german but not shepherd), and noway of enforcing Boolean
conditions. \\
* LSI can be viewed as soft clustering by interpreting SOFT CLUSTERING each dimension of the
reduced space as a cluster and the value that a document has on that dimension
as its fractional membership in that cluster. \\


\section{Evaluation of cluster labeling algorithm WCC}

The experiments in this study conducted several objective measures to validate the performance of the described algorithm \gls{WCC}. \\

Evaluating the quality of cluster labeling is a non-trivial task and the most suitable evaluation is judged by user's experiences. \\

The algorithm was implemented in Java on a a 64-bit Windows 7 Ultimate workstation with an Intel Core 2 Duo P8600 @ 2,40 GHz using 4.00 GB RAM. \\

\subsubsection{Document set}
In order to test a cluster labeling algorithm, we need a test data set containing predefined categories. \gls{WCC} is tested using the evaluation data set with three predefined categories selected from the whole collection used for evaluation of \gls{LSA} implementation. \\
 
\begin{table}[]
\centering
\begin{tabular}{l l  c}
\hline
\multicolumn{3}{c}{Document set overview}\\
\hline
Category Id & Category & Document \# \\
\hline
1 & session, connection & 5 \\
2 & server & 5 \\
3 & publication, workflow & 5 \\
\hline
\end{tabular}
\caption[Document set overview]{Overview of the constructed document set}
\end{table}

The document set used for evaluation consists of 3 categories, and 15 documents - each category has 5 documents, and the documents have different length. Table~\ref{eval:data_set} gives an overview of the constructed data sets. The document preprocessing involves parsing and stop word removal according to standard stop word lists. \\


\textbf{Data preprocessing} \\
The evaluation document set needs to be preprocessed before it can be used in the experiments. Analogous to the preprocessing used for the whole data set, punctuation and stop-words were removed. Terms in texts were not stemmed, as they are used as cluster labels, and stemming the terms would lead to worse user experience. \\

% TODO: correct text wrapping - fix column width for document text
%\begin{landscape}
\begin{table} [!ht]
\centering
\begin{tabular} { c c p{10cm} }
\hline
Doc. Id & Cat. Id & Document \\
\hline
1 & 1 & The session that is created while the connection is opened is also known as the connection session. \\
2 & 1 & The sessions of the connected clients will be closed and no more content changes are possible. \\
3 & 1 & The previous code fragment shows how a second session is created from an existing connection. \\
4 & 1 & Multiple sessions show their greatest potential in trusted applications which receive help in restricting user views while maintaining a shared cache.  \\
5 & 1 & Having opened connection, all actions are executed on behalf of the single user whose credentials where provided when logging in. \\
\hline
6 & 2 & The state of the Master Live Server must be younger than the state of the Slave Live Server.  \\
7 & 2 & The rewrite module checks the availability of the requested file and, in the negative case, passes the request on to the Active Delivery Server. \\
8 & 2 & The directory layout of the Active Delivery Server has changed as well as the format of the configuration file. \\
9 & 2 & The CoreMedia Content Server is a central component that manages the content repository and the user authorization. \\
10 & 2 & The CoreMedia Content Management Server is the production system used to create and administrate content. \\
\hline
11 & 3 & If the database does not allow to take online-backups ensure that all publications are finished and that the last publication was successful. \\
12 & 3 & The third task in the workflow aims to check if the change set is empty. Then, no publication is necessary and the workflow can be finished.  \\
13 & 3 & This element is used to define which information should be shown in the columns of the workflow list at the left side of the workflow window. \\
14 & 3 & Publication will be executed when finishing the task after all resources in the change set have been approved. \\
15 & 3 & The CoreMedia Workflow installation comes with four predefined workflows which cover the publication of resources. \\
\hline
\end{tabular}
\caption[Document collection used for WCC testing]{Document collection used for WCC testing}
\label{eval:data_set}
\end{table}


\subsection{Cluster labeling evaluation}

To evaluate the quality of WCC, an evaluation document set of 15 texts in 3 categories was prepared. \\  

To evaluate the WCC algorithm, we prepared a document set with author-defined keywords. The evaluation idea is to cluster the documents, label the resulting clustering, compute f1-f4, and measure to which extent the identified topic labels appear in the keyword list of at least one document of the associated cluster. ??? \\
To measure the extent, standard precision and recall values can be chosen. Note, however, that the precision value is more important for the scenario tested in this work since we nominate only ????(\textbf{TODO}: test with different "l" ???) words per cluster label to be presented to users. \\

Figure 3.15 shows precision and recall curves for \gls{WCC} depending on the number of extracted words per label. The values are averaged over 10 retries of the experiment, each time redrawing a new set of documents. The precision curve shows that one to five cluster labels can be identified at very high precision rates. (??check with my results?) \\


