\chapter{Evaluation}
\label{chapter:evaluation}

Evaluation in \gls{IR} is based on measures. As this work implements techniques for information retrieval, several wide-spread measures are described next, used commonly in \gls{IR} systems to evaluate performance. Then, the implemented tests are presented, and finally the obtained test results are discussed. \\


\section{Measures for evaluation}
In order to perform an evaluation of \gls{IR} systems, one needs in the simplest case:
\begin{itemize}
\item a document collection, 
\item a test suite of information needs, or a set of queries,
\item a set of relevance judgments to define for each query-document pair weather the document is relevant or not relevant for the query posed.
\end{itemize} 

Relevance is assessed relative to an information need, not a query. A document is considered relevant if it addresses the users' information need, not only if it contains all the words given in the query~(Manning et al.~\cite{Mann08}). When short queries (composed of one word for example) are posted, it is difficult to define the information need of users. But nevertheless, the user has one, and the retrieved search results are evaluated based on his or her information need. \\

The two most frequent and basic measures for evaluation of \gls{IR} systems are \textit{precision} and \textit{recall}~\cite{IRbook2008}. Recall shows the ability of a retrieval system to present all relevant items, while precision shows its ability to present only relevant items. For the simple case of a system which returns a result set of document to a query, they are defined:  \\

\begin{equation}
\mbox{\textbf{precision}} = \frac{\mbox{number of relevant documents retrieved}} {\mbox{total number of relevant documents}}
\end{equation}


\begin{equation}
\mbox{\textbf{recall}} = \frac{\mbox{number of relevant documents retrieved}} {\mbox{total number of retrieved documents}}
\end{equation}

\section{Document set used for evaluation}
In order to evaluate the retrieval and visualization of main concepts in a document corpus by the Tag cloud summarizer tool, and cluster labeling by \gls{WCC}, an evaluation document set was prepared, which contains documents in three categories (see Appendix~\ref{document_set_tables}) and includes 15 documents which have different lengths. Table~\ref{eval:data_set} gives an overview of the constructed data sets. Due to time constraints, only a simple usability evaluation was made, using the queries from Appendix~\ref{document_set_tables}.


\section{Evaluation of the Tag cloud summarizer}
The evaluation of Tag cloud summarizer tool was done by collecting user feedback. Due to time constraints only a limited number of feedback was collected. 3 users were interviewed. Based on the prepared document set for evaluation and the queries, they were asked for feedback about the usability of the tool: \\
\begin{itemize}
\item Is the tool intuitive to use?
\item Does it visualize the main concepts in the text collection based on the query made?
\item Does it bring value to a search application?

\end{itemize}

% query publication workflow - tag cloud
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{img/publication_workflow} 
	\caption[Tag cloud generated]{Tag cloud generated using the evaluation document set from the results to a query "publication workflow"}
\label{publication_workflow}
\end{figure}

All users found the tool intuitive to use. Concerning its usefulness, the main drawback pointed out was that in its current implementation, the Tag cloud summarizer produces a text cloud, that is a cloud of words, which do not link to the documents where they occur. It remains for future work to improve the prototype, and to implement hyperlinks from the tags in the cloud, which return the set of documents where the corresponding tag has the highest frequencies. Concerning the visualization of main concepts, which are closest to a query, 2 out of 3 users stated that not all displayed tags are connected to the query posted. However, if a larger document set for evaluation is used, the retrieved concepts should be closer to the queries, and the generated tag cloud therefore more precise. Testing with a large document set remains as well for future work. 

\section{Evaluation of cluster labeling algorithm WCC}
Evaluating the quality of cluster labeling is difficult as no standard benchmark collection is available for this purpose. To measure the quality of labels, one needs for a comparison a document clusterings that have been labeled by humans. Therefore, such evaluations are subjective and not easily reproducible. \\

Using the document set for evaluation, three clusters were prepared from each category using k-means clustering. Then, the standard precision and recall were measured based on a varying number of terms per label (from 1 to 5). When evaluating cluster labeling, precision is more important than recall for users, as only a limited number of terms are nominated for labels. \\


These results are not similar to the ones obtained by Meyer zu Eissen in~\cite{eissen_phd_2007}, who reports precision above 0.6 for 1 - 5 extracted terms per label. The worse results are presumably due to the very small document set used in this work for evaluation, as well as the different document collection used as compared to the one from~\cite{eissen_phd_2007}.  

