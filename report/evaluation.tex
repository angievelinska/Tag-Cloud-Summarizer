\chapter{Evaluation}
\label{chapter:evaluation}

Evaluation in \gls{IR} is based on measures. As this work implements techniques for information retrieval, several wide-spread measures are described next, used commonly in \gls{IR} systems to evaluate performance. Then, the implemented tests are presented, and finally are discussed the obtained test results. \\


\section{Measures for evaluation}
If an evaluation of \gls{IR} systems is performed, one needs in the simplest case:
\begin{itemize}
\item a document collection, 
\item a test suite of information needs, or a set of queries,
\item a set of relevance judgments to define for each query-document pair weather the document is relevant or not relevant for the query posed.
\end{itemize} 

Relevance is assessed relative to an information need, not a query. A document is considered relevant if it addresses the users' information need, not only if it contains all the words given in the query~(Manning et al.~\cite{Mann08}). When short queries (composed of one word for example) are posted, it is difficult to define the information need of users. But nevertheless, the user has one, and the retrieved search results are evaluated based on his or her information need. \\

The two most frequent and basic measures for evaluation of \gls{IR} systems are \textit{precision} and \textit{recall}. For the simple case of a system which returns a result set of document to a query, they are defined:  \\

\subsubsection{Precision}
\subsubsection{Recall}


\subsection{Evaluation of LSA}
Presision: \\
Recall: \\

\subsection{Evaluation of algorithms for cluster labeling}
User evaluation of cluster labeling algorithm \\
? F-measure:\\

\section{Experimental evaluation}
Example for presentation and evaluation of results, IR system:~\cite{SpamFilteringLSI2008} \\
\begin{itemize}
\item Data set
\item Experimental setup
\item Analysis of data matrices
\item True/false positive rates
\item Feature reduction
\end{itemize}

\section{LSA implementation}
\label{sec:implementation:lsa_impl}
\gls{LSA} was applied to a collection of 11818 words in 4000 documents, all of which describe CoreMedia \gls{CMS} 5.2. The algorithm took 63552 ms for preprocessing and indexing of the whole document collection. \\

For the implementation of LSA this work uses the open LSA library which is part of Semantic Spaces Project\cite{S-Space}. It is developed at the Natural Language Processing Group at the University of California at Berkley (UCLA)\footnote{\url{http://code.google.com/p/airhead-research/}, accessed December, 2010}.\\

The real difficulty of LSA is to find out how many dimensions to remove - the problem of dimensionality.\\

TODO:test if inluding only terms that occur in more than one document improved \gls{LSA} performance with respect to generating precise tag clouds.\\

\subsubsection{Data set}
It is a challenge by itself to come up with a sensible evaluation set for an IR implementation. ... Define here precision, recall, measures , how to measure LSA performance with  diff. k, clusters, cluster labelling. \\

The document collection consists of guides and manuals about CoreMedia \gls{CMS} 5.2.\\

This work has been carried out using documents from the domain of CoreMedia \gls{CMS} 5.2 as an evaluation set, developed at CoreMedia AG, Hamburg. \\


\subsubsection{Data preprocessing}
Stop words and punctuation are removed. The data model used in DocMachine stores the documents as XML files, and CoreMedia \gls{CMS} Unified API\footnote{\url{https://documentation.coremedia.com/servlet/content/241548?language=en&version=5.2&book=coremedia:///cap/content/241548}} is used to strip the markup, in order to access the plain text~(listing~\ref{strip_markup}). \\

\section{Cluster labeling}

The experiments in this study conducted several objective measures to validate the performance of the described algorithm \gls{WCC}. \\

Evaluating the quality of cluster labeling is a non-trivial task and the most suitable evaluation is judged by user's experiences. \\

The algorithm was implemented in Java on a a 64-bit Windows 7 Ultimate workstation with an Intel Core 2 Duo P8600 @ 2,40 GHz using 4.00 GB RAM. \\

\subsubsection{Document set}
In order to test a cluster labeling algorithm, we need a test data set containing predefined categories. \gls{WCC} is tested using the evaluation data set with three predefined categories selected from the whole collection used for evaluation of \gls{LSA} implementation. \\
 
\begin{table}[]
\centering
\begin{tabular}{l l  c}
\hline
\multicolumn{3}{c}{Document set overview}\\
\hline
Category Id & Category & Document \# \\
\hline
1 & session, connection & 5 \\
2 & server & 5 \\
3 & publication, workflow & 5 \\
\hline
\end{tabular}
\caption[Document set overview]{Overview of the constructed document set}
\end{table}

The document set used for evaluation consists of 3 categories, and 15 documents - each category has 5 documents, and the documents have different length. Table~\ref{eval:data_set} gives an overview of the constructed data sets. The document preprocessing involves parsing and stop word removal according to standard stop word lists. \\


\textbf{Data preprocessing} \\
The evaluation document set needs to be preprocessed before it can be used in the experiments. Analogous to the preprocessing used for the whole data set, punctuation and stop-words were removed. Terms in texts were not stemmed, as they are used as cluster labels, and stemming the terms would lead to worse user experience. \\

% TODO: correct text wrapping - fix column width for document text
%\begin{landscape}
\begin{table} [!ht]
\centering
\begin{tabular} { c c p{10cm} }
\hline
Doc. Id & Cat. Id & Document \\
\hline
1 & 1 & The session that is created while the connection is opened is also known as the connection session. \\
2 & 1 & The sessions of the connected clients will be closed and no more content changes are possible. \\
3 & 1 & The previous code fragment shows how a second session is created from an existing connection. \\
4 & 1 & Multiple sessions show their greatest potential in trusted applications which receive help in restricting user views while maintaining a shared cache.  \\
5 & 1 & Having opened connection, all actions are executed on behalf of the single user whose credentials where provided when logging in. \\
\hline
6 & 2 & The state of the Master Live Server must be younger than the state of the Slave Live Server.  \\
7 & 2 & The rewrite module checks the availability of the requested file and, in the negative case, passes the request on to the Active Delivery Server. \\
8 & 2 & The directory layout of the Active Delivery Server has changed as well as the format of the configuration file. \\
9 & 2 & The CoreMedia Content Server is a central component that manages the content repository and the user authorization. \\
10 & 2 & The CoreMedia Content Management Server is the production system used to create and administrate content. \\
\hline
11 & 3 & If the database does not allow to take online-backups ensure that all publications are finished and that the last publication was successful. \\
12 & 3 & The third task in the workflow aims to check if the change set is empty. Then, no publication is necessary and the workflow can be finished.  \\
13 & 3 & This element is used to define which information should be shown in the columns of the workflow list at the left side of the workflow window. \\
14 & 3 & Publication will be executed when finishing the task after all resources in the change set have been approved. \\
15 & 3 & The CoreMedia Workflow installation comes with four predefined workflows which cover the publication of resources. \\
\hline
\end{tabular}
\caption[Document collection used for WCC testing]{Document collection used for WCC testing}
\label{eval:data_set}
\end{table}


\subsection{Cluster labeling evaluation}

To evaluate the quality of WCC, Popescul and Ungar’s method, and the standard
keyword extraction algorithm RSP, we downloaded 150 documents from the digital library Citeseer, each of which containing author-defined keywords.

To evaluate the WCC algorithm, we prepared a document set with author-defined keywords. The evaluation idea is to cluster the documents, label the resulting clustering, compute f1-f4, and measure to which extent the identified topic labels appear in the keyword list of at least one document of the associated cluster. To measure the extent, standard precision and recall values can be chosen. Note, however, that the precision value is more important in our scenario since usually only a few words (about one to five) are presented to a user. \\

The clustering was done on the results from queries used for evaluation of \gls{LSA} (??).  \\
The clustering was done on the prepared data set~\ref{eval:data_set}, used for evaluation of the \gls{IR} methods implemented in this work. \\

Figure 3.15 shows precision and recall curves for \gls{WCC} depending on the number of extracted words per label. The values are averaged over 10 retries of the experiment, each time redrawing a new set of documents. The precision curve shows that one to five cluster labels can be identified at very high precision rates. (??check with my results?) \\


\section{LSA evaluation from IR book}
• The computational cost of the SVD is significant. This has been the biggest obstacle to the widespread adoption to LSI. One approach to this obstacle is to build the LSI representation on a
randomly sampled subset of the documents in the collection, following which the remaining documents are “folded in” as detailed with Equation (18.21). \\
• As we reduce k, recall tends to increase, as expected. \\
• Most surprisingly, a value of k in the low hundreds can actually increase
precision on some query benchmarks. This appears to suggest that for a
suitable value of k, LSI addresses some of the challenges of synonymy. \\
• LSIworks best in applicationswhere there is little overlap between queries
and documents. \\
The experiments also documented some modes where LSI failed to match
the effectiveness of more traditional indexes and score computations. Most
notably (and perhaps obviously), LSI shares two basic drawbacks of vector
space retrieval: there is no good way of expressing negations (find documents
that contain german but not shepherd), and noway of enforcing Boolean
conditions. \\
LSI can be viewed as soft clustering by interpreting SOFT CLUSTERING each dimension of the
reduced space as a cluster and the value that a document has on that dimension
as its fractional membership in that cluster. \\

