\chapter{Evaluation}
\label{chapter:evaluation}

Evaluation in \gls{IR} is based on measures. As this work implements techniques for information retrieval, several wide-spread measures are described next, used commonly in \gls{IR} systems to evaluate performance. Then, the implemented tests are presented, and finally the obtained test results are discussed. \\


\section{Measures for evaluation}
In order to perform an evaluation of \gls{IR} systems, one needs in the simplest case:
\begin{itemize}
\item a document collection, 
\item a test suite of information needs, or a set of queries,
\item a set of relevance judgments to define for each query-document pair weather the document is relevant or not relevant for the query posed.
\end{itemize} 

Relevance is assessed relative to an information need, not a query. A document is considered relevant if it addresses the users' information need, not only if it contains all the words given in the query~(Manning et al.~\cite{Mann08}). When short queries (composed of one word for example) are posted, it is difficult to define the information need of users. But nevertheless, the user has one, and the retrieved search results are evaluated based on his or her information need. \\

The two most frequent and basic measures for evaluation of \gls{IR} systems are \textit{precision} and \textit{recall}~\cite{IRbook2008}. Recall shows the ability of a retrieval system to present all relevant items, while precision shows its ability to present only relevant items. For the simple case of a system which returns a result set of document to a query, they are defined:  \\

\begin{equation}
\mbox{\textbf{precision}} = \frac{\mbox{number of relevant documents retrieved}} {\mbox{total number of relevant documents}}
\end{equation}


\begin{equation}
\mbox{\textbf{recall}} = \frac{\mbox{number of relevant documents retrieved}} {\mbox{total number of retrieved documents}}
\end{equation}

\section{Document set used for evaluation}
In order to evaluate the retrieval and visualization of main concepts in a document corpus by the Tag cloud summarizer tool, and cluster labeling by \gls{WCC}, an evaluation document set was prepared, which contains documents in three categories (see Appendix~\ref{document_set_tables}). Due to time constraints, only a simple usability evaluation was made, using the queries from Appendix~\ref{document_set_tables}.


\section{Evaluation of the Tag cloud summarizer}

\subsubsection{Dimensionality reduction parameter in LSA}
Certain factors influence the performance of LSA, as already discussed in section~\ref{sec:lsa:factors_infl_lsa}. 
\begin{itemize}
\item \textit{Preprocessing} - before constructing a semantic space using \gls{LSA}, stop words were removed from the evaluation document set. Stemming is not done, as terms from the semantic space are later used for the tag cloud generation.
Stop words and punctuation are removed. The data model used in DocMachine stores the documents as XML files, and CoreMedia \gls{CMS} Unified API\footnote{\url{https://documentation.coremedia.com/servlet/content/241548?language=en&version=5.2&book=coremedia:///cap/content/241548}} is used to strip the markup, in order to access the plain text~(listing~\ref{doc_preprocessing}). \\

\item \textit{Frequency matrix transformation} - log-entropy weight functions~(section~\ref{lsa:log-entropy-section}) were chosen, due to their better performance as compared to other measures (reported by Nakov et al.~\cite{Nakov_weightfunctions}).
\item \textit{Dimensionality reduction parameter \textbf{k}} - it is defined based on the experiments carried out using the document set for evaluation from CoreMedia \gls{CMS} domain, described in section~\ref{sec:eval:doc_set_lsa}. 

\end{itemize}


\subsubsection{LSA evaluation results}
LSI can be viewed as soft clustering by interpreting SOFT CLUSTERING each dimension of the
reduced space as a cluster and the value that a document has on that dimension
as its fractional membership in that cluster. \\


\section{Evaluation of cluster labeling algorithm WCC}

The experiments in this study conducted several objective measures to validate the performance of the described algorithm \gls{WCC}. \\

Evaluating the quality of cluster labeling is a non-trivial task and the most suitable evaluation is judged by user's experiences. \\

The algorithm was implemented in Java on a a 64-bit Windows 7 Ultimate workstation with an Intel Core 2 Duo P8600 @ 2,40 GHz using 4.00 GB RAM. \\

\subsubsection{Document set}
In order to test a cluster labeling algorithm, we need a test data set containing predefined categories. \gls{WCC} is tested using the evaluation data set with three predefined categories selected from the whole collection used for evaluation of \gls{LSA} implementation. \\
 

The document set used for evaluation consists of 3 categories, and 15 documents - each category has 5 documents, and the documents have different length. Table~\ref{eval:data_set} gives an overview of the constructed data sets. The document preprocessing involves parsing and stop word removal according to standard stop word lists. \\


\textbf{Data preprocessing} \\
The evaluation document set needs to be preprocessed before it can be used in the experiments. Analogous to the preprocessing used for the whole data set, punctuation and stop-words were removed. Terms in texts were not stemmed, as they are used as cluster labels, and stemming the terms would lead to worse user experience. \\

\subsection{Cluster labeling evaluation}

To evaluate the quality of WCC, an evaluation document set of 15 texts in 3 categories was prepared. \\  

To evaluate the WCC algorithm, we prepared a document set with author-defined keywords. The evaluation idea is to cluster the documents, label the resulting clustering, compute f1-f4, and measure to which extent the identified topic labels appear in the keyword list of at least one document of the associated cluster. ??? \\
To measure the extent, standard precision and recall values can be chosen. Note, however, that the precision value is more important for the scenario tested in this work since we nominate only ????(\textbf{TODO}: test with different "l" ???) words per cluster label to be presented to users. \\



